---
title: "PageIndexã‚’OSSã¨è‡ªä½œPythonã‚³ãƒ¼ãƒ‰ã§å‹•ã‹ã™"
emoji: "ğŸŒ²"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [LLM, RAG, Python, OSS, PageIndex]
published: false
---

ã“ã®è¨˜äº‹ã§ã¯ã€[PageIndex](https://pageindex.ai/)ã‚’OSSã¨Pythonã§è©¦ã—ã¦ã¿ãŸå†…å®¹ã‚’æ›¸ã„ã¦ã„ãã¾ã™ã€‚

## PageIndexã¨ã¯
[PageIndex](https://pageindex.ai/)ã¯ã€ç« ç«‹ã¦ã«ãªã£ã¦ã„ã‚‹PDFã‚„Markdownãªã©ã®æ–‡æ›¸ã‚’JSONå½¢å¼ã®ãƒ„ãƒªãƒ¼æ§‹é€ ã«å¤‰æ›ã—ã€ãã®JSONã‚’ã‚‚ã¨ã«æƒ…å ±ã®æ¤œç´¢ã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚
æ¦‚å¿µã¨ã—ã¦ã®è©³ç´°ã¯[ã€Œãƒ™ã‚¯ãƒˆãƒ«DBä¸è¦ã€ãªRAGæ‰‹æ³•ã€ŒPageIndexã€ã‚’è§£èª¬](https://zenn.dev/knowledgesense/articles/2895f9adc8d802)ã®è¨˜äº‹ãŒã‚ã‹ã‚Šã‚„ã™ã„ã¨æ€ã„ã¾ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€å®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ãŸã€OSSã¨ã—ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹PageIndexã¨ç‹¬è‡ªã®Pythonã‚³ãƒ¼ãƒ‰ã‚’çµ„ã¿åˆã‚ã›ãŸå†…å®¹ã‚’æ›¸ã„ã¦ã„ãã¾ã™ã€‚

:::message
PageIndexã§ã¯æœ‰æ–™ã®ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨OSSç‰ˆãŒã‚ã‚Šã¾ã™ã€‚
OSSç‰ˆã§ã¯ãƒ„ãƒªãƒ¼æ§‹é€ ã«ã™ã‚‹ã¾ã§ã®ã‚³ãƒ¼ãƒ‰ãŒå…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ãŒã€æ¤œç´¢éƒ¨åˆ†ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã®ã§å®Ÿè£…ã¯è‡ªåˆ†ã§è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ä»Šå›ã®è¨˜äº‹ã§ã¯OSSç‰ˆã§ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’ä½œæˆã—ã€è‡ªä½œã®Pythonã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢éƒ¨åˆ†ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚
:::

## å®Ÿè£…ã‚³ãƒ¼ãƒ‰
å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã«å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚

## æœ¨æ§‹é€ ã®å‡ºåŠ›æ–¹æ³•
- [PageIndex OSSãƒªãƒã‚¸ãƒˆãƒª](https://github.com/VectifyAI/PageIndex/tree/main/tutorials/tree-search)ã«ã‚ã‚‹é€šã‚Šã®æ‰‹é †ã§æœ¨æ§‹é€ ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

### Pythonç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone git@github.com:VectifyAI/PageIndex.git
cd PageIndex

# ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆãƒ»æœ‰åŠ¹åŒ–ï¼ˆè‡ªåˆ†ã®ç’°å¢ƒã«åˆã‚ã›ã¦ãã ã•ã„ï¼‰

# ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip3 install --upgrade -r requirements.txt
```

# OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
```bash
touch .env
echo "CHATGPT_API_KEY=your_openai_key_here">> .env
```

### ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é…ç½®
ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å®Ÿéš›ã«æ¤œç´¢ã—ãŸã„PDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é…ç½®ã—ã¾ã™ã€‚
ï¼ˆä»Šå›ã¯ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ï¼‰

### æœ¨æ§‹é€ ã®å‡ºåŠ›
```bash
python3 run_pageindex.py --pdf_path /path/to/your/document.pdf --model gpt-4.1-mini-2025-04-14
```
:::message
modelã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ä½¿ç”¨ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
ç¾åœ¨ã¯OpenAIã®GPTã®ã¿å¯¾å¿œã—ã¦ã„ã‚‹æ¨¡æ§˜ã§ã™ã€‚
ã¡ãªã¿ã«gpt-5ã¯ã¾ã å¯¾å¿œã—ã¦ã„ãªã„ãªã©ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
:::

:::message alert
è‡ªåˆ†ã¯æœ€åˆã«gpt-4.1-nanoã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œã—ã¾ã—ãŸãŒã€ã©ã†ã‚„ã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚„æ–‡æ›¸ã®é•·ã•ãªã©ã«ã‚ˆã£ã¦ã¯ã†ã¾ãã„ã‹ãªã„ã‚ˆã†ã§ã€ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã—ãŸã€‚ gpt-4.1-mini-2025-04-14ã‚’æŒ‡å®šã—ãŸã¨ã“ã‚æ­£å¸¸ã«å‹•ä½œã—ã¾ã—ãŸã€‚
:::

### å‡ºåŠ›çµæœ
å®Ÿéš›ã®å‡ºåŠ›çµæœã®**æŠœç²‹**ã§ã™ã€‚
JSONå½¢å¼ã§ãƒ„ãƒªãƒ¼æ§‹é€ ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

```json
{
  "doc_name": "Attention_is_all_you_need.pdf",
  "structure": [
    {
      "title": "Preface",
      "start_index": 1,
      "end_index": 1,
      "node_id": "0000",
      "summary": "The partial document presents the introduction and abstract of the paper \"Attention Is All You Need,\" which proposes the Transformer, a novel neural network architecture for sequence transduction tasks. Unlike previous models relying on recurrent or convolutional networks, the Transformer is based entirely on attention mechanisms, eliminating the need for recurrence and convolutions. The paper highlights the model's superior performance on machine translation benchmarks (WMT 2014 English-to-German and English-to-French), achieving state-of-the-art BLEU scores with significantly reduced training time and computational resources. Additionally, the Transformer demonstrates strong generalization to other tasks such as English constituency parsing. The document also includes author contributions and affiliations, emphasizing the collaborative effort behind the development and implementation of the Transformer and its associated codebases."
    },
    {
      "title": "Introduction",
      "start_index": 2,
      "end_index": 2,
      "node_id": "0001",
      "summary": "The partial document introduces the limitations of recurrent neural networks (RNNs), particularly their sequential nature that hinders parallelization and efficiency in sequence modeling tasks like language modeling and machine translation. It highlights the rise of attention mechanisms as a way to model dependencies regardless of sequence distance, though typically used alongside recurrent models. The document then proposes the Transformer architecture, which eliminates recurrence entirely and relies solely on self-attention to capture global dependencies, enabling greater parallelization and improved translation quality. It contrasts the Transformer with prior convolutional and recurrent models, emphasizing its constant-time operation for relating sequence positions and introducing Multi-Head Attention to address resolution issues. The background section reviews related models that reduce sequential computation and discusses the use of self-attention in various NLP tasks. Finally, the document outlines the standard encoder-decoder framework for sequence transduction, setting the stage for a detailed description of the Transformer model architecture."
    },
    {
      "title": "Model Architecture",
      "start_index": 2,
      "end_index": 3,
      "nodes": [
        {
          "title": "Encoder and Decoder Stacks",
          "start_index": 3,
          "end_index": 5,
          "node_id": "0004",
          "summary": "The partial document provides a detailed explanation of the Transformer model architecture, focusing on its encoder and decoder stacks, attention mechanisms, feed-forward networks, and embedding strategies. It describes the encoder and decoder as composed of six identical layers, each with sub-layers involving multi-head self-attention and position-wise feed-forward networks, enhanced by residual connections and layer normalization. The decoder includes an additional multi-head attention sub-layer over the encoder output and employs masking to maintain autoregressive properties. The attention section elaborates on the Scaled Dot-Product Attention mechanism, explaining its computation, the rationale for scaling, and its efficiency compared to additive attention. It further introduces Multi-Head Attention, which projects queries, keys, and values into multiple subspaces to capture diverse information, detailing the mathematical formulation and parameter dimensions. The document also outlines the three applications of multi-head attention within the Transformer: encoder-decoder attention, encoder self-attention, and decoder self-attention with masking. Additionally, it covers the position-wise feed-forward networks applied identically at each position with two linear transformations and ReLU activation. Finally, it discusses the use of learned embeddings for input and output tokens, sharing weights between embedding layers and the pre-softmax linear transformation to generate predicted token probabilities."
        },
        {
          "title": "Attention",
          "start_index": 5,
          "end_index": 6,
          "nodes": [
            {
              "title": "Scaled Dot-Product Attention",
              "start_index": 6,
              "end_index": 4,
              "node_id": "0006",
              "summary": "The partial document discusses key concepts or instructions related to a specific topic, focusing on essential points or steps. It likely outlines procedures, definitions, or explanations aimed at providing clarity and understanding of the subject matter."
            },
            {
              "title": "Multi-Head Attention",
              "start_index": 4,
              "end_index": 4,
              "node_id": "0007",
              "summary": "The partial document explains the concepts of Scaled Dot-Product Attention and Multi-Head Attention. It details how Scaled Dot-Product Attention computes attention weights by taking the dot product of queries and keys, scaling by the square root of the key dimension to prevent large magnitude values, and applying a softmax function to obtain weighted values. The document contrasts this method with additive attention, highlighting the efficiency and practical advantages of scaled dot-product attention. It then introduces Multi-Head Attention, which involves linearly projecting queries, keys, and values multiple times into lower-dimensional spaces and performing parallel attention operations. This approach allows the model to jointly attend to information from different representation subspaces, enhancing performance."
            },
            {
              "summary": "The partial document presents the introduction and abstract of the paper \"Attention Is All You Need,\" which proposes the Transformer, a novel neural network architecture for sequence transduction tasks. Unlike previous models relying on recurrent or convolutional networks, the Transformer is based entirely on attention mechanisms, eliminating the need for recurrence and convolutions. The paper highlights the model's superior performance on machine translation benchmarks (WMT 2014 English-to-German and English-to-French), achieving state-of-the-art BLEU scores with significantly reduced training time and computational resources. Additionally, the Transformer demonstrates strong generalization to other tasks such as English constituency parsing. The document also includes author contributions and affiliations, emphasizing the collaborative effort behind the development and implementation of the Transformer and its associated codebases."
            }
          ]
        }
      ]
    }
  ]
}
```

## æ¤œç´¢éƒ¨åˆ†ã®å®Ÿè£…
ã“ã“ã‹ã‚‰ã¯OSSç‰ˆã®GitHubãƒªãƒã‚¸ãƒˆãƒªã«å®Ÿè£…ã¯è¼‰ã›ã‚‰ã‚Œã¦ã„ãªã„ãŸã‚ã€è‡ªåˆ†ã§å®Ÿè£…ã—ãŸæ¤œç´¢éƒ¨åˆ†ã®ã‚³ãƒ¼ãƒ‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
ãŸã ã—ã€[ã“ã¡ã‚‰](https://github.com/VectifyAI/PageIndex/tree/main/tutorials/tree-search)ã«ãƒ„ãƒªãƒ¼æ§‹é€ ã‹ã‚‰ã®æ¤œç´¢ã®è€ƒãˆæ–¹ã¯è¼‰ã›ã‚‰ã‚Œã¦ã„ã¾ã™ã®ã§ã€ãã¡ã‚‰ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

TODO: ãƒªãƒã‚¸ãƒˆãƒªURLå·®ã—æ›¿ãˆ
:::message
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã“ã®è¨˜äº‹ã‚ˆã†ã«ç°¡ç•¥åŒ–ã—ãŸã‚³ãƒ¼ãƒ‰ã§ã™ã€‚
å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã¯[ã“ã¡ã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/VectifyAI/PageIndex/tree/main/tutorials/tree-search)ã‚’ã”è¦§ãã ã•ã„ã€‚ 
:::
https://github.com/VectifyAI/PageIndex/tree/main/tutorials/tree-search


### å®Ÿè£…ã®æµã‚Œ
1. ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’JSONã‹ã‚‰Pythonã®ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹
2. LLMã«è©•ä¾¡ã•ã›ã‚‹é–¢æ•°ã‚’ä½œæˆã™ã‚‹
3. å¹…å„ªå…ˆæ¢ç´¢ã—ã¤ã¤LLMã®æŒ‡ç¤ºã«å¾“ã†
4. ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã—ã¦å®Ÿè¡Œã™ã‚‹

### 1. ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’JSONã‹ã‚‰Pythonã®ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹
ã¾ãšã¯æœ€å°é™ã® `TreeNode` ã‚¯ãƒ©ã‚¹ã¨ã€JSON ã‹ã‚‰ãƒ„ãƒªãƒ¼ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚ãƒ‘ã‚¹æƒ…å ±ã¨ã‚µãƒãƒªãƒ¼ã‚’æŒãŸã›ã‚‹ã¨ã€å¾Œç¶šã®è¡¨ç¤ºãŒåˆ†ã‹ã‚Šã‚„ã™ããªã‚Šã¾ã™ã€‚

```python
from dataclasses import dataclass, field
from pathlib import Path
import json
from typing import Dict, List, Sequence, Tuple

@dataclass
class TreeNode:
    node_id: str
    title: str
    summary: str
    depth: int
    path_titles: Tuple[str, ...]
    children: List["TreeNode"] = field(default_factory=list)

    def pretty_path(self) -> str:
        return " > ".join(self.path_titles)

    def short_summary(self, max_chars: int = 160) -> str:
        if len(self.summary) <= max_chars:
            return self.summary
        return self.summary[: max_chars - 3] + "..."


def load_structure(path: Path) -> Tuple[List[TreeNode], Dict[str, TreeNode]]:
    payload = json.loads(path.read_text())
    roots: List[TreeNode] = []
    lookup: Dict[str, TreeNode] = {}

    def build(node_payload: dict, parent_path: Tuple[str, ...], depth: int) -> TreeNode:
        node = TreeNode(
            node_id=node_payload.get("node_id", ""),
            title=node_payload.get("title", ""),
            summary=node_payload.get("summary", ""),
            depth=depth,
            path_titles=parent_path + (node_payload.get("title", ""),),
        )
        lookup[node.node_id] = node
        for child_payload in node_payload.get("nodes", []):
            child = build(child_payload, node.path_titles, depth + 1)
            node.children.append(child)
        return node

    for root_payload in payload.get("structure", []):
        roots.append(build(root_payload, tuple(), 0))

    return roots, lookup
```

---

## 2. å€™è£œãƒãƒ¼ãƒ‰ã‚’ LLM ã«è©•ä¾¡ã•ã›ã‚‹
LLM ã«æŠ•ã’ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œã‚Šã€JSON å½¢å¼ã§è¿”ç­”ã—ã¦ã‚‚ã‚‰ã†æœ€å°ä¾‹ã§ã™ã€‚ã“ã“ã§ã¯ OpenAI ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚

```python
import openai

PROMPT_TEMPLATE = """
You are given a query and several candidate nodes from a document tree.
Select nodes that are relevant and tell us which node to expand next.

Query: {query}

Candidates:
{candidates}

Return JSON with keys "thinking", "relevant_nodes", and "expand".
"""

def render_candidates(nodes: Sequence[TreeNode]) -> str:
    lines = []
    for node in nodes:
        lines.append(f"[{node.node_id}] depth={node.depth} title={node.title}")
        lines.append(f"summary: {node.short_summary(110)}")
    return "\n".join(lines)


def ask_llm(query: str, nodes: Sequence[TreeNode], *, model: str, api_key: str) -> dict:
    prompt = PROMPT_TEMPLATE.format(query=query, candidates=render_candidates(nodes))
    client = openai.OpenAI(api_key=api_key)
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    return json.loads(response.choices[0].message.content)
```

---

## 3. å¹…å„ªå…ˆã§æ¢ç´¢ã—ã¤ã¤ LLM ã®æŒ‡ç¤ºã«å¾“ã†
å®Ÿéš›ã® `tree_search_example.py` ã«ã‚ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç•¥åŒ–ã—ã€LLM ã®æŒ‡ç¤ºã«å¿œã˜ã¦æ¢ç´¢ã‚’ç¶šã‘ã‚‹éƒ¨åˆ†ã‚’æœ€å°åŒ–ã—ã¾ã™ã€‚å¤±æ•—æ™‚ã«ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹å˜ç´”ãªä¾‹ã§ã™ã€‚

```python
from collections import deque
import re


def keyword_score(query: str, node: TreeNode) -> float:
    terms = re.findall(r"\w+", query.lower())
    haystack = f"{node.title} {node.summary}".lower()
    hits = sum(1 for term in terms if term in haystack)
    return hits / max(1, len(terms))


def tree_search(query: str, roots: Sequence[TreeNode], *, model: str, api_key: str, max_turns: int = 3) -> List[TreeNode]:
    queue: deque[TreeNode] = deque(roots)
    seen: set[str] = set()
    selected: List[TreeNode] = []

    for _ in range(max_turns):
        batch: List[TreeNode] = []
        while queue and len(batch) < 4:
            node = queue.popleft()
            if node.node_id in seen:
                continue
            seen.add(node.node_id)
            batch.append(node)

        if not batch:
            break

        try:
            decision = ask_llm(query, batch, model=model, api_key=api_key)
        except Exception:
            # LLM ãŒè½ã¡ãŸã‚‰ç°¡æ˜“çš„ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            batch.sort(key=lambda n: keyword_score(query, n), reverse=True)
            selected.extend(batch[:2])
            continue

        for item in decision.get("relevant_nodes", []):
            node = next((n for n in batch if n.node_id == item.get("node_id")), None)
            if node and node not in selected:
                selected.append(node)

        for node_id in decision.get("expand", []):
            node = next((n for n in batch if n.node_id == node_id), None)
            if node:
                queue.extend(node.children)

    return selected
```

---

## 4. å®Ÿè¡Œä¾‹
æœ€å¾Œã«ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã‚’ç”¨æ„ã—ã¦å‹•ã‹ã—ã¾ã™ã€‚API ã‚­ãƒ¼ãŒç„¡ã„å ´åˆã¯ `tree_search` å†…ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ã¿ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

```python
import argparse

def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("query")
    parser.add_argument("--structure", default="toy_structure.json")
    parser.add_argument("--model", default="gpt-4.1-nano-2025-04-14")
    parser.add_argument("--api-key")
    args = parser.parse_args()

    roots, _ = load_structure(Path(args.structure))
    selections = tree_search(
        args.query,
        roots,
        model=args.model,
        api_key=args.api_key or "",
    )

    if not selections:
        print("No nodes selected.")
        return

    print("Selected nodes:")
    for node in selections:
        print(f"- {node.pretty_path()}: {node.short_summary()}")


if __name__ == "__main__":
    main()
```

## ã¾ã¨ã‚
ä»¥ä¸ŠãŒPageIndexã®OSSç‰ˆã‚’åˆ©ç”¨ã—ã€è‡ªä½œã®Pythonã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢éƒ¨åˆ†ã‚’å®Ÿè£…ã—ãŸä¾‹ã«ãªã‚Šã¾ã™ã€‚
PageIndexè‡ªä½“ã¯ã¾ã æ–°ã—ã„æŠ€è¡“ã§ã‚ã‚Šã€OSSç‰ˆã‚‚ç™ºå±•é€”ä¸Šãªéƒ¨åˆ†ãŒå¤šã„ã§ã™ãŒã€ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’åˆ©ç”¨ã—ãŸæ¤œç´¢æ‰‹æ³•ã¨ã—ã¦ã¯é¢ç™½ã„æ–¹æ³•è«–ã ã¨æ€ã„ã¾ã™ã€‚
ã‚ã‚‹ç¨‹åº¦ç« ç«‹ã¦ã«ãªã£ã¦ã„ã‚‹ãªã©ã€ä½¿ãˆã‚‹å ´é¢ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯é™ã‚‰ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã‚ˆã‹ã£ãŸã‚‰è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
